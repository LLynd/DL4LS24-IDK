{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning_lite in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.8.6)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightning_lite) (1.26.2)\n",
      "Requirement already satisfied: torch>=1.9.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightning_lite) (1.12.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]>2021.06.0->lightning_lite) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\dell\\appdata\\roaming\\python\\python39\\site-packages (from lightning_lite) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightning_lite) (4.9.0)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightning_lite) (0.11.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]>2021.06.0->lightning_lite) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fsspec[http]>2021.06.0->lightning_lite) (3.9.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lightning-utilities!=0.4.0,>=0.3.0->lightning_lite) (58.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning_lite) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning_lite) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning_lite) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning_lite) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning_lite) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->lightning_lite) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->lightning_lite) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->lightning_lite) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->lightning_lite) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->lightning_lite) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install lightning_lite\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lightning_lite import seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping  # ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "seed_everything(10, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Users/Dell/Documents/Deep_learning_for_life_science/Projekt_zaliczeniowy/DL4LS24-IDK-main/DL4LS24-IDK-main/data/train/cell_data.h5ad\"\n",
    "### input Your path\n",
    "\n",
    "anndata = ad.read_h5ad(path)\n",
    "anndata.layers\n",
    "anndata.X = anndata.layers['exprs'] # FIX!\n",
    "\n",
    "df=anndata.obs\n",
    "counts=anndata.layers['counts']\n",
    "exprs=anndata.layers['exprs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending information from exprs to dataframe for prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236791, 40)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cols_counts=['counts'+str(i) for i in range(40)]\n",
    "list_cols_exprs=['exprs'+str(i) for i in range(40)]\n",
    "df_counts = pd.DataFrame(data=counts, columns=list_cols_counts,index=df.index)\n",
    "df_exprs = pd.DataFrame(data=exprs, columns=list_cols_exprs,index=df.index)\n",
    "df_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236791, 79)\n",
      "Index(['image', 'sample_id', 'ObjectNumber', 'Pos_X', 'Pos_Y', 'area',\n",
      "       'major_axis_length', 'minor_axis_length', 'eccentricity', 'width_px',\n",
      "       'height_px', 'acquisition_id', 'SlideId', 'Study', 'Box.Description',\n",
      "       'Position', 'SampleId', 'Indication', 'BatchId', 'SubBatchId', 'ROI',\n",
      "       'ROIonSlide', 'includeImage', 'flag_no_cells', 'flag_no_ROI',\n",
      "       'flag_total_area', 'flag_percent_covered', 'small_cell', 'celltypes',\n",
      "       'flag_tumor', 'PD1_pos', 'Ki67_pos', 'cleavedPARP_pos', 'GrzB_pos',\n",
      "       'tumor_patches', 'distToCells', 'CD20_patches', 'Batch', 'cell_labels',\n",
      "       'exprs0', 'exprs1', 'exprs2', 'exprs3', 'exprs4', 'exprs5', 'exprs6',\n",
      "       'exprs7', 'exprs8', 'exprs9', 'exprs10', 'exprs11', 'exprs12',\n",
      "       'exprs13', 'exprs14', 'exprs15', 'exprs16', 'exprs17', 'exprs18',\n",
      "       'exprs19', 'exprs20', 'exprs21', 'exprs22', 'exprs23', 'exprs24',\n",
      "       'exprs25', 'exprs26', 'exprs27', 'exprs28', 'exprs29', 'exprs30',\n",
      "       'exprs31', 'exprs32', 'exprs33', 'exprs34', 'exprs35', 'exprs36',\n",
      "       'exprs37', 'exprs38', 'exprs39'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df1=df.copy()\n",
    "result = pd.concat([df1, df_exprs], axis=1)\n",
    "print(result.shape)\n",
    "print(result.columns)\n",
    "\n",
    "df=result.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepering targets and removing unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['image', 'sample_id', 'ObjectNumber', 'Pos_X', 'Pos_Y', 'width_px',\n",
    "       'height_px', 'acquisition_id', 'SlideId', 'Study', 'Box.Description',\n",
    "       'Position', 'SampleId', 'Indication', 'BatchId', 'SubBatchId', 'ROI',\n",
    "       'ROIonSlide', 'includeImage', 'flag_no_cells', 'flag_no_ROI',\n",
    "       'flag_total_area', 'flag_percent_covered', 'small_cell', 'celltypes',\n",
    "       'flag_tumor', 'PD1_pos', 'Ki67_pos', 'cleavedPARP_pos', 'GrzB_pos',\n",
    "       'tumor_patches', 'distToCells', 'CD20_patches', 'Batch']  # List of columns to remove\n",
    "\n",
    "# Remove columns\n",
    "df = df.drop(cols_to_remove, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['cell_labels']\n",
    "encoder = LabelEncoder()\n",
    "y_encoded1 = encoder.fit_transform(y)\n",
    "df_y = pd.DataFrame(data=y_encoded1, columns=['target'],index=df.index)\n",
    "# print(df_y.shape, df_y, df['cell_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df, df_y], axis=1)\n",
    "#df=result.copy()\n",
    "#df = df.drop(['cell_labels'], axis=1)\n",
    "df = result.copy()\n",
    "df = df.drop(['cell_labels'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pytorch dataset from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "                dataframe with our data\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = self.data.iloc[idx]\n",
    "        features = torch.tensor(sample.iloc[:-1].values, dtype=torch.float32)\n",
    "        target = torch.tensor(int(sample.iloc[-1]), dtype=torch.long)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into a train and test set, create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47358, 45) (189433, 45) (236791, 45)\n"
     ]
    }
   ],
   "source": [
    "test_proportion = 0.2\n",
    "batch_size = 30\n",
    "\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "df_test = shuffled_df.iloc[:int(test_proportion * (shuffled_df.shape[0]))]\n",
    "df_train = shuffled_df.iloc[int(test_proportion * (shuffled_df.shape[0])):]\n",
    "print(df_test.shape, df_train.shape, shuffled_df.shape)\n",
    "train_dataset = MyDataset(df_train)\n",
    "test_dataset = MyDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting pytorch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU...\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use GPU acceleration.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_block(nn.Module):\n",
    "    def __init__(self, a, b, dropout):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(a, b)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "        self.batchnorm = nn.BatchNorm1d(b)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers_list, dropout):\n",
    "        super().__init__()\n",
    "        self.my_modules = nn.ModuleList([Linear_block(layers_list[i-1], layers_list[i], dropout) for i in range(1, len(layers_list))])\n",
    "        self.clasification_head = nn.Linear(layers_list[-1], 14)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.my_modules:\n",
    "            x = module(x)\n",
    "        x = self.clasification_head(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(tensor, num_classes = 14):\n",
    "    # Create a zero tensor with the desired shape\n",
    "    one_hot = torch.zeros(tensor.size(0), num_classes)\n",
    "    # Use scatter_ to fill the one-hot tensor\n",
    "    one_hot.scatter_(1, tensor.unsqueeze(1), 1)\n",
    "    return one_hot\n",
    "\n",
    "def calculate_accuracy(model, test_dataloader, device = device):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        corect_preds = 0\n",
    "        total_preds = 0\n",
    "        clases_total = torch.zeros((14))\n",
    "        corect_pre_class = torch.zeros((14))\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim = 1)\n",
    "            corect = torch.sum(preds == labels)\n",
    "            corect_preds += corect.item()\n",
    "            total_preds += labels.shape[0]\n",
    "            one_hot_labels = one_hot_encoding(labels)\n",
    "            one_hot_preds = one_hot_encoding(preds)\n",
    "            clases_total += torch.sum(one_hot_labels, dim = 0)\n",
    "            corect_pre_class += torch.sum(one_hot_labels * one_hot_preds, dim = 0)\n",
    "\n",
    "        print(f'Corect total: {corect_preds}, total number of all predictions: {total_preds}, accuracy: {corect_preds/total_preds}')\n",
    "        print(f'Corect per class, total of number datapoints per class, accuracy per class: \\n {corect_pre_class} \\n {clases_total} \\n {corect_pre_class/clases_total}')\n",
    "\n",
    "def train_model(model, lr, train_dataloader, epochs, weight = None, device = device):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight = weight)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs).to(device)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 0:\n",
    "                print(f'Epoch {epoch}, batch {i}, loss: {loss}')\n",
    "        print(f'End of epoch {epoch}, avarage loss = {running_loss/len(train_dataloader)}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0, loss: 2.6461777687072754\n",
      "Epoch 0, batch 1000, loss: 1.912638783454895\n",
      "Epoch 0, batch 2000, loss: 1.9587945938110352\n",
      "Epoch 0, batch 3000, loss: 1.9894936084747314\n",
      "Epoch 0, batch 4000, loss: 1.9851605892181396\n",
      "Epoch 0, batch 5000, loss: 2.1805219650268555\n",
      "Epoch 0, batch 6000, loss: 1.903768539428711\n",
      "End of epoch 0, avarage loss = 1.9837992471074266\n",
      "Epoch 1, batch 0, loss: 1.8371540307998657\n",
      "Epoch 1, batch 1000, loss: 1.7950774431228638\n",
      "Epoch 1, batch 2000, loss: 2.173959255218506\n",
      "Epoch 1, batch 3000, loss: 1.9870713949203491\n",
      "Epoch 1, batch 4000, loss: 1.8436479568481445\n",
      "Epoch 1, batch 5000, loss: 1.8052631616592407\n",
      "Epoch 1, batch 6000, loss: 2.0422000885009766\n",
      "End of epoch 1, avarage loss = 1.9194675678123299\n"
     ]
    }
   ],
   "source": [
    "weight_for_cross_entropy = torch.tensor([1, 1, 1, 1, 5, 5, 1, 1, 6, 1, 1, 1, 7, 1]).float()\n",
    "# bez różnych wag dla różnych klas model miał 0% accuracy dla 4 różnych klas. W ten sposób zmuszam go do tego, żeby choć trochę klasyfikował każdą klasę\n",
    "\n",
    "My_MLP = MLP([44, 100, 100, 100], 0.1)\n",
    "My_MLP_trained = train_model(My_MLP, 0.001, train_dataloader, 2, weight = weight_for_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set:\n",
      "Corect total: 171326, total number of all predictions: 189433, accuracy: 0.9044147535012379\n",
      "Corect per class, total of number datapoints per class, accuracy per class: \n",
      " tensor([ 3628.,  4650.,  6352., 15840.,  3073.,  1694.,  9949., 16110.,   698.,\n",
      "         5292.,  4093., 93461.,   953.,  5533.]) \n",
      " tensor([ 3884.,  5439., 10214., 16861.,  3809.,  2966., 12495., 17766.,   920.,\n",
      "         5813.,  4544., 97096.,  1324.,  6302.]) \n",
      " tensor([0.9341, 0.8549, 0.6219, 0.9394, 0.8068, 0.5711, 0.7962, 0.9068, 0.7587,\n",
      "        0.9104, 0.9007, 0.9626, 0.7198, 0.8780])\n",
      "accuracy on the test set:\n",
      "Corect total: 42829, total number of all predictions: 47358, accuracy: 0.9043667384602391\n",
      "Corect per class, total of number datapoints per class, accuracy per class: \n",
      " tensor([  909.,  1171.,  1673.,  3921.,   778.,   427.,  2496.,  3996.,   150.,\n",
      "         1263.,  1062., 23405.,   247.,  1331.]) \n",
      " tensor([  978.,  1371.,  2625.,  4181.,   959.,   733.,  3143.,  4403.,   202.,\n",
      "         1380.,  1171., 24364.,   335.,  1513.]) \n",
      " tensor([0.9294, 0.8541, 0.6373, 0.9378, 0.8113, 0.5825, 0.7941, 0.9076, 0.7426,\n",
      "        0.9152, 0.9069, 0.9606, 0.7373, 0.8797])\n"
     ]
    }
   ],
   "source": [
    "#z barhcnormem:, z wagami roznymi dla roznych klas\n",
    "print('Accuracy on the training set:')\n",
    "calculate_accuracy(My_MLP_trained, train_dataloader)\n",
    "print('accuracy on the test set:')\n",
    "calculate_accuracy(My_MLP_trained, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
